{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "\n",
    "In this week's exercise session we will learn how to use Machine Learning methods to solve regression problems. In particular, we will focus on linear regression. \n",
    "\n",
    "First let us import the libraries we will be using during this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 The regression problem\n",
    "\n",
    "We have seen in class that regression refers to predicting continous values for a given sample. A nice example could be predicting the first salary of a student after graduation given how diligent they were with attending machine learning exercise sessions. We were introduced to the \"linear regression method\". \n",
    "\n",
    "**Q: How does a regression problem differ from a classification problem?**\n",
    "\n",
    "**Q: Why is the linear regression model a linear model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Load and inspect the data\n",
    "\n",
    "This week, using the linear regression method, we will analyze the Boston house prices data set and predict costs based on properties such as per capita crime rate by town, pupil-teacher ratio by town, etc.\n",
    "\n",
    "We load the data and split it such that 80% and 20% are training and test data, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zedrichu/.local/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# get the data set and print a description\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()\n",
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data sample matrix X: (475, 12)\n",
      "The number of data samples is N: 475\n",
      "The number of features is D: 12\n",
      "\n",
      "Shape of the labels vector y: (475,)\n",
      "\n",
      "First data sample in X: [6.320e-03 1.800e+01 2.310e+00 5.380e-01 6.575e+00 6.520e+01 4.090e+00\n",
      " 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00]\n",
      "First label in y: 24.0\n"
     ]
    }
   ],
   "source": [
    "X = boston_dataset[\"data\"]\n",
    "y = boston_dataset[\"target\"]\n",
    "\n",
    "# remove categorical feature\n",
    "X = np.delete(X, 3, axis=1)\n",
    "# removing second mode\n",
    "ind = y<40\n",
    "X = X[ind,:]\n",
    "y = y[ind]\n",
    "\n",
    "print(\"Shape of the data sample matrix X:\", X.shape)\n",
    "\n",
    "N = X.shape[0]\n",
    "D = X.shape[1]\n",
    "print(\"The number of data samples is N:\", N)\n",
    "print(\"The number of features is D:\", D)\n",
    "\n",
    "\n",
    "print(\"\\nShape of the labels vector y:\", y.shape)\n",
    "\n",
    "\n",
    "print(\"\\nFirst data sample in X:\", X[0,:])\n",
    "print(\"First label in y:\", y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first exercise is to split the data into training and test sets. Let's set aside 80% of the data for training and 20% for testing. \n",
    "\n",
    "Your steps should be the following ones:\n",
    "1. Generate a vector of indices from 0 to N-1 (with N being the number of data samples) (Hint: you can use the function np.arange()\n",
    "2. Shuffle the indices (hint: you can use np.random.shuffle(). Look up how to use this function!)\n",
    "3. Select 80% of the indices. We have coded this for you but make sure you understand what these lines are doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "\n",
    "# Step 1:\n",
    "indices = np.arange(0, N)\n",
    "\n",
    "# Step 2:\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Step 3:\n",
    "X_train    = X[indices[0:int(N*split_ratio)],:] \n",
    "y_train    = y[indices[0:int(N*split_ratio)]] \n",
    "# Split the test data using the remaining indices!\n",
    "X_test     = X[indices[int(N*split_ratio):], :]\n",
    "y_test     = y[indices[int(N*split_ratio):]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes of `X_train`, `y_train`, `X_test`, `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(380, 12)\n",
      "(380,)\n",
      "(95, 12)\n",
      "(95,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# The shapes should be (380, 12), (380,), (95, 12), (95,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, we normalize the data such that each feature has zero mean and unit standard deviation. (Since you have not seen this in the lectures yet, we have provided the code for you. Study this well, we will be doing a lot of normalization during the exercise sessions!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make mean 0 and std dev 1 of the data.\n",
    "'''\n",
    "def normalize(X):\n",
    "    mu    = np.mean(X,0,keepdims=True)\n",
    "    std   = np.std(X,0,keepdims=True) \n",
    "    X     = (X-mu)/std\n",
    "    return X, mu, std\n",
    "\n",
    "#Use train stats for normalizing test set\n",
    "X_train,mu_train,std_train = normalize(X_train)\n",
    "X_test = (X_test-mu_train)/std_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Attribute $X_4$ vs Price $y$')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploratory analysis of the data. Have a look at the distribution of prices vs features\n",
    "\n",
    "feature = 4\n",
    "plt.scatter(X_train[:,feature], y_train)\n",
    "plt.xlabel(f\"Attribute $X_{feature}$\")\n",
    "plt.ylabel(\"Price $y$\")\n",
    "plt.title(f\"Attribute $X_{feature}$ vs Price $y$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q: Using the code above, explore the relation between different features and the house prices. Describe what you see. Can you identify any trends?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Closed-form solution for linear regression\n",
    "\n",
    "\n",
    "The linear regression method has a closed form, analytical solution, as we have also seen in class.\n",
    "\n",
    "$$ \\mathbf{w^*} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\n",
    "\n",
    "\n",
    "Now let's code the analytical solution in the function `get_w_analytical` and to obtain the weight parameters $\\mathbf{w}$. Tip: You may want to use the function np.linalg.pinv(). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_analytical(X_train,y_train):\n",
    "    \"\"\"\n",
    "    compute the weight parameters w\n",
    "    \"\"\"\n",
    "    # compute w via the analytical solution\n",
    "    w = np.linalg.inv(X_train.T @ X_train) @ (X_train.T) @ y_train\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess our method's performance, we'll be using the mean squared error (MSE). \n",
    "\n",
    "$$ MSE = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "where our prediction $\\hat{y}_i = \\mathbf{x}_i^T \\cdot \\mathbf{w} $.\n",
    "\n",
    "\n",
    "\n",
    "Let's code this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_loss(w, X_train, y_train,X_test,y_test):\n",
    "    \n",
    "    loss_train = 1 / X_train.shape[0] * np.sum((X_train @ w - y_train) ** 2)\n",
    "    #loss_train = np.mean((y_train - X_train @ w) ** 2)\n",
    "    loss_test = 1 / X_test.shape[0] * np.sum((X_test @ w - y_test) ** 2)\n",
    "    print(\"The training loss is {}. The test loss is {}.\".format(loss_train, loss_test))\n",
    "    \n",
    "    return loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss is 436.8437185477003. The test loss is 437.3294638920944.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "437.3294638920944"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's test our code!\n",
    "w_ana = get_w_analytical(X_train,y_train)\n",
    "get_loss(w_ana, X_train,y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What is the shape of the analytical weights?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Adding a bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error of 400 is quite high! Note however that, in contrast to what we have seen in the lectures, we did not use any bias term $\\text{w}^{(0)}$. Let's see whether we can reduce the error by including a bias term.\n",
    "\n",
    "First, let's look more closely at what happens without a bias term. Formally, without a bias term, we are fitting a hyperplane that always passes through the origin. This is because our predictions can be expressed as\n",
    "$$ \\hat{y}_i = \\text{w}^{(1)}x_i^{(1)} +  \\text{w}^{(2)}x_i^{(2)} + ... + \\text{w}^{(12)}x_i^{(12)} $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ \\hat{y}_i = \\mathbf{x}_i^T \\cdot \\mathbf{w} $$ \n",
    "\n",
    "Therefore, when $\\mathbf{x}_i=\\mathbf{0}$, $\\hat{y}_i= 0$, no matter what values $\\mathbf{w}$ takes. That's not ideal!\n",
    "\n",
    "Note: If you are confused about the transpose operation in $\\hat{y}_i = \\mathbf{x}_i^T \\cdot \\mathbf{w}$ above, here are the shapes of the matrices that are being multiplied:  \n",
    "* $\\mathbf{w}$ is $D \\times 1$ \n",
    "* $\\mathbf{x}_i$ is $D \\times 1$ (A reminder: The entire data $\\mathbf{X}$ is $N \\times D$, but when we select a single data sample from it, we express it as a column vector!) \n",
    "* The result $\\hat{y}_i$ is $1 \\times 1$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introducing the bias term:**\n",
    "        \n",
    "It would be a lot nicer if our predicted hyperplane didn't always have to pass through the origin. In math words:\n",
    "\n",
    "$$ \\hat{y}_i = \\text{w}^{(0)} + \\text{w}^{(1)}x_i^{(1)} +  \\text{w}^{(2)}x_i^{(2)} + ... + \\text{w}^{(12)}x_i^{(12)} $$\n",
    "\n",
    "Here, the $\\text{w}^{(0)}$ is the y-intercept. When $\\mathbf{x}_i=\\mathbf{0}$, $y_i= \\text{w}^{(0)}$. Neat!\n",
    "\n",
    "\n",
    "To handle this, we can add a column of 1s as a feature in our data $\\mathbf{X}$. This way, we could just say that the last feature $x_i^{(0)} = 1$ and\n",
    "\n",
    "$$ \\hat{y}_i = \\text{w}^{(0)}\\cdot 1 + \\text{w}^{(1)}x_i^{(1)} +  \\text{w}^{(2)}x_i^{(2)} + ... + \\text{w}^{(12)}x_i^{(12)} $$\n",
    "\n",
    "$$ \\hat{y}_i = \\text{w}^{(0)}x_i^{(0)} + \\text{w}^{(1)}x_i^{(1)} +  \\text{w}^{(2)}x_i^{(2)} + ... + \\text{w}^{(12)}x_i^{(12)} $$\n",
    "\n",
    "$$ \\hat{y}_i = \\mathbf{x}_i^T \\cdot \\mathbf{w} $$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "And we can keep using the same analytical solution formula as above! So by adding a column of 1s as the last feature of $\\mathbf{X}$, and running the analytical solution, we will find a $\\mathbf{w}$ with 13 features instead of 12. The last feature of the weights $\\mathbf{w}^{(0)}$ will be the bias term. This way, we wouldn't have to change any of the functions we wrote above.\n",
    "\n",
    "So let's get to it! Fill in the function below to append a bias term to the data matrices $\\mathbf{X}$. Your steps should be the following:\n",
    "1. Create a numpy array that is a column of 1s. It's shape should be $N \\times 1$.\n",
    "2. Concatenate the ones column with the data matrix. Hint: use np.concatenate. Be careful what axis you specify!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_bias_term(X_train):\n",
    "\n",
    "    ones_column = np.ones(shape=(X_train.shape[0],1))\n",
    "    X_train_bias = np.concatenate((ones_column, X_train), axis=1)\n",
    "    print(X_train_bias.shape)\n",
    "    return X_train_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(380, 13)\n",
      "(95, 13)\n",
      "The training loss is 10.399481428585586. The test loss is 11.7924703471258.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.7924703471258"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bias = append_bias_term(X_train)\n",
    "X_test_bias = append_bias_term(X_test)\n",
    "\n",
    "w_ana = get_w_analytical(X_train_bias,y_train)\n",
    "\n",
    "get_loss(w_ana, X_train_bias,y_train, X_test_bias,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your loss should be around 10. That's much better, no?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4. Solution using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model has an analytical solution, but we can also get the weight parameters $\\mathbf{w}$ numerically, e.g., via gradient descent. We will be using this approach to complete the function `get_w_numerical` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us code the gradient of the MSE loss, as we saw in class:\n",
    "$$\\nabla R = \\frac{2}{N}\\sum_i^N(\\mathbf{x}_i^T \\mathbf{w}-y_i)\\cdot \\mathbf{x}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gradient(X, y, w):\n",
    "    \"\"\"computes the gradient of the empirical risk, R\"\"\"\n",
    "\n",
    "    N = X.shape[0]\n",
    "    w = w.reshape(-1,1)\n",
    "    y = y.reshape(-1,1)\n",
    "    # print(X.shape) # (380, 13)\n",
    "    # print(w.shape) # (13, 1)\n",
    "    # print((X @ w).shape) # (380, 1) \n",
    "    # print((X @ w - y).shape) # (380, 1)\n",
    "    # (380,1) * (380,13) - broadcasting \n",
    "\n",
    "    grad = 2/N * np.sum((X @ w - y) * X, axis=0)\n",
    "\n",
    "    return grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13,)\n"
     ]
    }
   ],
   "source": [
    "#Let's create a random w to test the function above.\n",
    "w = np.random.normal(0, 1e-1, X_train_bias.shape[1])\n",
    "grad = find_gradient(X_train_bias, y_train, w)\n",
    "print(grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can write the function that finds $\\mathbf{w}$ using gradient descent. Recall that gradient descent works via the update\n",
    "$$w_{k} \\leftarrow w_{k-1} - \\eta \\nabla R$$\n",
    "\n",
    "where $\\eta$ is the learning rate and $k$ is the iteration number.\n",
    "\n",
    "Fill in the function below to update $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_numerical(X_train,y_train,X_test,y_test,epochs,lr):\n",
    "    \"\"\"compute the weight parameters w\"\"\"\n",
    "    \n",
    "    # initialize the weights\n",
    "    w = np.random.normal(0, 1e-1, X_train.shape[1]) # --> (13, )\n",
    "    \n",
    "    # iterate a given number of epochs over the training data\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        w = w - lr * find_gradient(X_train, y_train, w)\n",
    "       \n",
    "        # Test every 500 epochs to see whether the training loss and test losses are going down.\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"\\nEpoch {1000+epoch}/{epochs}\")\n",
    "            get_loss(w, X_train,y_train, X_test,y_test)\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1000/100000\n",
      "The training loss is 474.0994715991583. The test loss is 540.1597031612055.\n",
      "\n",
      "Epoch 1500/100000\n",
      "The training loss is 70.26081201033145. The test loss is 74.05502984809713.\n",
      "\n",
      "Epoch 2000/100000\n",
      "The training loss is 19.478759907461214. The test loss is 21.893089391514234.\n",
      "\n",
      "Epoch 2500/100000\n",
      "The training loss is 12.364413170103116. The test loss is 14.498690363743084.\n",
      "\n",
      "Epoch 3000/100000\n",
      "The training loss is 11.228108864326257. The test loss is 13.248335825781473.\n",
      "\n",
      "Epoch 3500/100000\n",
      "The training loss is 10.949924855475777. The test loss is 12.886398881159476.\n",
      "\n",
      "Epoch 4000/100000\n",
      "The training loss is 10.821163176090234. The test loss is 12.68631504730498.\n",
      "\n",
      "Epoch 4500/100000\n",
      "The training loss is 10.735633924738886. The test loss is 12.539824544708617.\n",
      "\n",
      "Epoch 5000/100000\n",
      "The training loss is 10.672246737571152. The test loss is 12.424799064863603.\n",
      "\n",
      "Epoch 5500/100000\n",
      "The training loss is 10.623619940042506. The test loss is 12.332536547439709.\n",
      "\n",
      "Epoch 6000/100000\n",
      "The training loss is 10.585624977842159. The test loss is 12.257558471226119.\n",
      "\n",
      "Epoch 6500/100000\n",
      "The training loss is 10.555498738273423. The test loss is 12.195912327252943.\n",
      "\n",
      "Epoch 7000/100000\n",
      "The training loss is 10.531292030996697. The test loss is 12.14467022837455.\n",
      "\n",
      "Epoch 7500/100000\n",
      "The training loss is 10.511601651083462. The test loss is 12.101645458023512.\n",
      "\n",
      "Epoch 8000/100000\n",
      "The training loss is 10.495404222227771. The test loss is 12.065191660687299.\n",
      "\n",
      "Epoch 8500/100000\n",
      "The training loss is 10.481944733370623. The test loss is 12.034056735605775.\n",
      "\n",
      "Epoch 9000/100000\n",
      "The training loss is 10.470659713096337. The test loss is 12.007277223938402.\n",
      "\n",
      "Epoch 9500/100000\n",
      "The training loss is 10.46112359273902. The test loss is 11.98410239444953.\n",
      "\n",
      "Epoch 10000/100000\n",
      "The training loss is 10.453010945094402. The test loss is 11.963939730969956.\n",
      "\n",
      "Epoch 10500/100000\n",
      "The training loss is 10.446069721195519. The test loss is 11.946315677073242.\n",
      "\n",
      "Epoch 11000/100000\n",
      "The training loss is 10.440102164356393. The test loss is 11.930847206290556.\n",
      "\n",
      "Epoch 11500/100000\n",
      "The training loss is 10.434951113665646. The test loss is 11.917221069429331.\n",
      "\n",
      "Epoch 12000/100000\n",
      "The training loss is 10.430490108745662. The test loss is 11.905178499466835.\n",
      "\n",
      "Epoch 12500/100000\n",
      "The training loss is 10.426616187422264. The test loss is 11.894503814330998.\n",
      "\n",
      "Epoch 13000/100000\n",
      "The training loss is 10.423244599695266. The test loss is 11.885015821805963.\n",
      "\n",
      "Epoch 13500/100000\n",
      "The training loss is 10.420304892011254. The test loss is 11.876561255252426.\n",
      "\n",
      "Epoch 14000/100000\n",
      "The training loss is 10.417737976764627. The test loss is 11.869009695228643.\n",
      "\n",
      "Epoch 14500/100000\n",
      "The training loss is 10.415493914580297. The test loss is 11.862249590045732.\n",
      "\n",
      "Epoch 15000/100000\n",
      "The training loss is 10.413530215949935. The test loss is 11.85618509863812.\n",
      "\n",
      "Epoch 15500/100000\n",
      "The training loss is 10.411810524358465. The test loss is 11.850733556434946.\n",
      "\n",
      "Epoch 16000/100000\n",
      "The training loss is 10.410303582199507. The test loss is 11.845823419291465.\n",
      "\n",
      "Epoch 16500/100000\n",
      "The training loss is 10.408982408446903. The test loss is 11.841392578982571.\n",
      "\n",
      "Epoch 17000/100000\n",
      "The training loss is 10.407823636652045. The test loss is 11.83738697111557.\n",
      "\n",
      "Epoch 17500/100000\n",
      "The training loss is 10.406806975768474. The test loss is 11.833759415932636.\n",
      "\n",
      "Epoch 18000/100000\n",
      "The training loss is 10.405914766243535. The test loss is 11.830468646660421.\n",
      "\n",
      "Epoch 18500/100000\n",
      "The training loss is 10.405131610937486. The test loss is 11.82747849042834.\n",
      "\n",
      "Epoch 19000/100000\n",
      "The training loss is 10.404444065557943. The test loss is 11.82475717443105.\n",
      "\n",
      "Epoch 19500/100000\n",
      "The training loss is 10.40384037701262. The test loss is 11.822276735730066.\n",
      "\n",
      "Epoch 20000/100000\n",
      "The training loss is 10.403310260792884. The test loss is 11.820012517416236.\n",
      "\n",
      "Epoch 20500/100000\n",
      "The training loss is 10.402844710493389. The test loss is 11.817942737168845.\n",
      "\n",
      "Epoch 21000/100000\n",
      "The training loss is 10.402435834051264. The test loss is 11.816048116818585.\n",
      "\n",
      "Epoch 21500/100000\n",
      "The training loss is 10.40207671239623. The test loss is 11.814311563540391.\n",
      "\n",
      "Epoch 22000/100000\n",
      "The training loss is 10.40176127704227. The test loss is 11.812717894906324.\n",
      "\n",
      "Epoch 22500/100000\n",
      "The training loss is 10.401484203794869. The test loss is 11.81125360131714.\n",
      "\n",
      "Epoch 23000/100000\n",
      "The training loss is 10.401240820246983. The test loss is 11.809906640376042.\n",
      "\n",
      "Epoch 23500/100000\n",
      "The training loss is 10.40102702512909. The test loss is 11.808666258623468.\n",
      "\n",
      "Epoch 24000/100000\n",
      "The training loss is 10.400839217890779. The test loss is 11.807522836757016.\n",
      "\n",
      "Epoch 24500/100000\n",
      "The training loss is 10.40067423714241. The test loss is 11.806467755046485.\n",
      "\n",
      "Epoch 25000/100000\n",
      "The training loss is 10.400529306790236. The test loss is 11.805493276143125.\n",
      "\n",
      "Epoch 25500/100000\n",
      "The training loss is 10.400401988866708. The test loss is 11.804592442892904.\n",
      "\n",
      "Epoch 26000/100000\n",
      "The training loss is 10.40029014219783. The test loss is 11.803758989109879.\n",
      "\n",
      "Epoch 26500/100000\n",
      "The training loss is 10.400191886167026. The test loss is 11.802987261558552.\n",
      "\n",
      "Epoch 27000/100000\n",
      "The training loss is 10.400105568934105. The test loss is 11.80227215164272.\n",
      "\n",
      "Epoch 27500/100000\n",
      "The training loss is 10.400029739552476. The test loss is 11.801609035509623.\n",
      "\n",
      "Epoch 28000/100000\n",
      "The training loss is 10.399963123500086. The test loss is 11.800993721458672.\n",
      "\n",
      "Epoch 28500/100000\n",
      "The training loss is 10.399904601201523. The test loss is 11.800422403697757.\n",
      "\n",
      "Epoch 29000/100000\n",
      "The training loss is 10.399853189172513. The test loss is 11.799891621622066.\n",
      "\n",
      "Epoch 29500/100000\n",
      "The training loss is 10.39980802346433. The test loss is 11.799398223903058.\n",
      "\n",
      "Epoch 30000/100000\n",
      "The training loss is 10.399768345125986. The test loss is 11.798939336772078.\n",
      "\n",
      "Epoch 30500/100000\n",
      "The training loss is 10.399733487437214. The test loss is 11.798512335966244.\n",
      "\n",
      "Epoch 31000/100000\n",
      "The training loss is 10.399702864695684. The test loss is 11.798114821875608.\n",
      "\n",
      "Epoch 31500/100000\n",
      "The training loss is 10.399675962368756. The test loss is 11.797744597492148.\n",
      "\n",
      "Epoch 32000/100000\n",
      "The training loss is 10.39965232844327. The test loss is 11.797399648814077.\n",
      "\n",
      "Epoch 32500/100000\n",
      "The training loss is 10.399631565827391. The test loss is 11.797078127404479.\n",
      "\n",
      "Epoch 33000/100000\n",
      "The training loss is 10.39961332567633. The test loss is 11.796778334842818.\n",
      "\n",
      "Epoch 33500/100000\n",
      "The training loss is 10.399597301529491. The test loss is 11.796498708841737.\n",
      "\n",
      "Epoch 34000/100000\n",
      "The training loss is 10.39958322416029. The test loss is 11.796237810830876.\n",
      "\n",
      "Epoch 34500/100000\n",
      "The training loss is 10.39957085705194. The test loss is 11.795994314834987.\n",
      "\n",
      "Epoch 35000/100000\n",
      "The training loss is 10.399559992423086. The test loss is 11.795766997495512.\n",
      "\n",
      "Epoch 35500/100000\n",
      "The training loss is 10.399550447736416. The test loss is 11.795554729103724.\n",
      "\n",
      "Epoch 36000/100000\n",
      "The training loss is 10.399542062631546. The test loss is 11.79535646553028.\n",
      "\n",
      "Epoch 36500/100000\n",
      "The training loss is 10.399534696230576. The test loss is 11.795171240950053.\n",
      "\n",
      "Epoch 37000/100000\n",
      "The training loss is 10.399528224771073. The test loss is 11.79499816127387.\n",
      "\n",
      "Epoch 37500/100000\n",
      "The training loss is 10.399522539526627. The test loss is 11.794836398209224.\n",
      "\n",
      "Epoch 38000/100000\n",
      "The training loss is 10.399517544980078. The test loss is 11.794685183881828.\n",
      "\n",
      "Epoch 38500/100000\n",
      "The training loss is 10.399513157218706. The test loss is 11.794543805957693.\n",
      "\n",
      "Epoch 39000/100000\n",
      "The training loss is 10.399509302524342. The test loss is 11.794411603212868.\n",
      "\n",
      "Epoch 39500/100000\n",
      "The training loss is 10.399505916134869. The test loss is 11.794287961503972.\n",
      "\n",
      "Epoch 40000/100000\n",
      "The training loss is 10.399502941156111. The test loss is 11.794172310098194.\n",
      "\n",
      "Epoch 40500/100000\n",
      "The training loss is 10.399500327605956. The test loss is 11.794064118326297.\n",
      "\n",
      "Epoch 41000/100000\n",
      "The training loss is 10.399498031574604. The test loss is 11.793962892526045.\n",
      "\n",
      "Epoch 41500/100000\n",
      "The training loss is 10.399496014486857. The test loss is 11.79386817324741.\n",
      "\n",
      "Epoch 42000/100000\n",
      "The training loss is 10.399494242453986. The test loss is 11.793779532694014.\n",
      "\n",
      "Epoch 42500/100000\n",
      "The training loss is 10.399492685704407. The test loss is 11.79369657237796.\n",
      "\n",
      "Epoch 43000/100000\n",
      "The training loss is 10.399491318083463. The test loss is 11.793618920967862.\n",
      "\n",
      "Epoch 43500/100000\n",
      "The training loss is 10.39949011661402. The test loss is 11.793546232312021.\n",
      "\n",
      "Epoch 44000/100000\n",
      "The training loss is 10.399489061110415. The test loss is 11.793478183620424.\n",
      "\n",
      "Epoch 44500/100000\n",
      "The training loss is 10.39948813383935. The test loss is 11.793414473791264.\n",
      "\n",
      "Epoch 45000/100000\n",
      "The training loss is 10.399487319221917. The test loss is 11.793354821868883.\n",
      "\n",
      "Epoch 45500/100000\n",
      "The training loss is 10.399486603571889. The test loss is 11.793298965621537.\n",
      "\n",
      "Epoch 46000/100000\n",
      "The training loss is 10.399485974865776. The test loss is 11.793246660228476.\n",
      "\n",
      "Epoch 46500/100000\n",
      "The training loss is 10.39948542254081. The test loss is 11.793197677066956.\n",
      "\n",
      "Epoch 47000/100000\n",
      "The training loss is 10.399484937317485. The test loss is 11.793151802590575.\n",
      "\n",
      "Epoch 47500/100000\n",
      "The training loss is 10.399484511043665. The test loss is 11.793108837291358.\n",
      "\n",
      "Epoch 48000/100000\n",
      "The training loss is 10.399484136557607. The test loss is 11.793068594738587.\n",
      "\n",
      "Epoch 48500/100000\n",
      "The training loss is 10.399483807567643. The test loss is 11.793030900688047.\n",
      "\n",
      "Epoch 49000/100000\n",
      "The training loss is 10.399483518546484. The test loss is 11.792995592256025.\n",
      "\n",
      "Epoch 49500/100000\n",
      "The training loss is 10.399483264638336. The test loss is 11.792962517152917.\n",
      "\n",
      "Epoch 50000/100000\n",
      "The training loss is 10.399483041577344. The test loss is 11.792931532971577.\n",
      "\n",
      "Epoch 50500/100000\n",
      "The training loss is 10.399482845615907. The test loss is 11.792902506526298.\n",
      "\n",
      "Epoch 51000/100000\n",
      "The training loss is 10.399482673461709. The test loss is 11.792875313238339.\n",
      "\n",
      "Epoch 51500/100000\n",
      "The training loss is 10.399482522222423. The test loss is 11.792849836564606.\n",
      "\n",
      "Epoch 52000/100000\n",
      "The training loss is 10.399482389357114. The test loss is 11.792825967465973.\n",
      "\n",
      "Epoch 52500/100000\n",
      "The training loss is 10.399482272633538. The test loss is 11.792803603912546.\n",
      "\n",
      "Epoch 53000/100000\n",
      "The training loss is 10.399482170090634. The test loss is 11.792782650422856.\n",
      "\n",
      "Epoch 53500/100000\n",
      "The training loss is 10.399482080005615. The test loss is 11.79276301763462.\n",
      "\n",
      "Epoch 54000/100000\n",
      "The training loss is 10.399482000864971. The test loss is 11.792744621904792.\n",
      "\n",
      "Epoch 54500/100000\n",
      "The training loss is 10.399481931339077. The test loss is 11.792727384936624.\n",
      "\n",
      "Epoch 55000/100000\n",
      "The training loss is 10.399481870259844. The test loss is 11.792711233431957.\n",
      "\n",
      "Epoch 55500/100000\n",
      "The training loss is 10.399481816601096. The test loss is 11.792696098766795.\n",
      "\n",
      "Epoch 56000/100000\n",
      "The training loss is 10.399481769461316. The test loss is 11.792681916688634.\n",
      "\n",
      "Epoch 56500/100000\n",
      "The training loss is 10.399481728048515. The test loss is 11.792668627033931.\n",
      "\n",
      "Epoch 57000/100000\n",
      "The training loss is 10.399481691666939. The test loss is 11.792656173464366.\n",
      "\n",
      "Epoch 57500/100000\n",
      "The training loss is 10.399481659705337. The test loss is 11.792644503220464.\n",
      "\n",
      "Epoch 58000/100000\n",
      "The training loss is 10.399481631626733. The test loss is 11.792633566891578.\n",
      "\n",
      "Epoch 58500/100000\n",
      "The training loss is 10.39948160695938. The test loss is 11.792623318200874.\n",
      "\n",
      "Epoch 59000/100000\n",
      "The training loss is 10.399481585288848. The test loss is 11.792613713804537.\n",
      "\n",
      "Epoch 59500/100000\n",
      "The training loss is 10.39948156625106. The test loss is 11.792604713104003.\n",
      "\n",
      "Epoch 60000/100000\n",
      "The training loss is 10.399481549526161. The test loss is 11.792596278070457.\n",
      "\n",
      "Epoch 60500/100000\n",
      "The training loss is 10.39948153483316. The test loss is 11.792588373080799.\n",
      "\n",
      "Epoch 61000/100000\n",
      "The training loss is 10.399481521925201. The test loss is 11.792580964764174.\n",
      "\n",
      "Epoch 61500/100000\n",
      "The training loss is 10.399481510585426. The test loss is 11.792574021858536.\n",
      "\n",
      "Epoch 62000/100000\n",
      "The training loss is 10.399481500623313. The test loss is 11.792567515076378.\n",
      "\n",
      "Epoch 62500/100000\n",
      "The training loss is 10.399481491871493. The test loss is 11.792561416979241.\n",
      "\n",
      "Epoch 63000/100000\n",
      "The training loss is 10.399481484182921. The test loss is 11.79255570186018.\n",
      "\n",
      "Epoch 63500/100000\n",
      "The training loss is 10.399481477428438. The test loss is 11.792550345633884.\n",
      "\n",
      "Epoch 64000/100000\n",
      "The training loss is 10.399481471494546. The test loss is 11.792545325733817.\n",
      "\n",
      "Epoch 64500/100000\n",
      "The training loss is 10.39948146628156. The test loss is 11.792540621015938.\n",
      "\n",
      "Epoch 65000/100000\n",
      "The training loss is 10.399481461701898. The test loss is 11.79253621166863.\n",
      "\n",
      "Epoch 65500/100000\n",
      "The training loss is 10.399481457678617. The test loss is 11.792532079128273.\n",
      "\n",
      "Epoch 66000/100000\n",
      "The training loss is 10.39948145414412. The test loss is 11.792528206000409.\n",
      "\n",
      "Epoch 66500/100000\n",
      "The training loss is 10.39948145103903. The test loss is 11.792524575985723.\n",
      "\n",
      "Epoch 67000/100000\n",
      "The training loss is 10.399481448311175. The test loss is 11.792521173810865.\n",
      "\n",
      "Epoch 67500/100000\n",
      "The training loss is 10.399481445914724. The test loss is 11.792517985163654.\n",
      "\n",
      "Epoch 68000/100000\n",
      "The training loss is 10.399481443809417. The test loss is 11.792514996632372.\n",
      "\n",
      "Epoch 68500/100000\n",
      "The training loss is 10.399481441959882. The test loss is 11.792512195648966.\n",
      "\n",
      "Epoch 69000/100000\n",
      "The training loss is 10.399481440335046. The test loss is 11.792509570435785.\n",
      "\n",
      "Epoch 69500/100000\n",
      "The training loss is 10.399481438907612. The test loss is 11.79250710995575.\n",
      "\n",
      "Epoch 70000/100000\n",
      "The training loss is 10.399481437653593. The test loss is 11.7925048038657.\n",
      "\n",
      "Epoch 70500/100000\n",
      "The training loss is 10.399481436551927. The test loss is 11.792502642472646.\n",
      "\n",
      "Epoch 71000/100000\n",
      "The training loss is 10.399481435584104. The test loss is 11.792500616692775.\n",
      "\n",
      "Epoch 71500/100000\n",
      "The training loss is 10.399481434733854. The test loss is 11.792498718013103.\n",
      "\n",
      "Epoch 72000/100000\n",
      "The training loss is 10.399481433986907. The test loss is 11.79249693845559.\n",
      "\n",
      "Epoch 72500/100000\n",
      "The training loss is 10.3994814333307. The test loss is 11.792495270543379.\n",
      "\n",
      "Epoch 73000/100000\n",
      "The training loss is 10.39948143275422. The test loss is 11.792493707269305.\n",
      "\n",
      "Epoch 73500/100000\n",
      "The training loss is 10.399481432247775. The test loss is 11.792492242066341.\n",
      "\n",
      "Epoch 74000/100000\n",
      "The training loss is 10.399481431802856. The test loss is 11.792490868779899.\n",
      "\n",
      "Epoch 74500/100000\n",
      "The training loss is 10.399481431411996. The test loss is 11.79248958164188.\n",
      "\n",
      "Epoch 75000/100000\n",
      "The training loss is 10.399481431068615. The test loss is 11.7924883752464.\n",
      "\n",
      "Epoch 75500/100000\n",
      "The training loss is 10.399481430766953. The test loss is 11.792487244526981.\n",
      "\n",
      "Epoch 76000/100000\n",
      "The training loss is 10.39948143050194. The test loss is 11.79248618473528.\n",
      "\n",
      "Epoch 76500/100000\n",
      "The training loss is 10.399481430269123. The test loss is 11.792485191421.\n",
      "\n",
      "Epoch 77000/100000\n",
      "The training loss is 10.399481430064592. The test loss is 11.792484260413245.\n",
      "\n",
      "Epoch 77500/100000\n",
      "The training loss is 10.399481429884908. The test loss is 11.792483387802896.\n",
      "\n",
      "Epoch 78000/100000\n",
      "The training loss is 10.399481429727054. The test loss is 11.792482569926237.\n",
      "\n",
      "Epoch 78500/100000\n",
      "The training loss is 10.399481429588377. The test loss is 11.792481803349466.\n",
      "\n",
      "Epoch 79000/100000\n",
      "The training loss is 10.399481429466547. The test loss is 11.792481084854298.\n",
      "\n",
      "Epoch 79500/100000\n",
      "The training loss is 10.39948142935952. The test loss is 11.792480411424417.\n",
      "\n",
      "Epoch 80000/100000\n",
      "The training loss is 10.399481429265496. The test loss is 11.792479780232803.\n",
      "\n",
      "Epoch 80500/100000\n",
      "The training loss is 10.399481429182895. The test loss is 11.79247918862982.\n",
      "\n",
      "Epoch 81000/100000\n",
      "The training loss is 10.39948142911033. The test loss is 11.7924786341321.\n",
      "\n",
      "Epoch 81500/100000\n",
      "The training loss is 10.399481429046576. The test loss is 11.792478114412098.\n",
      "\n",
      "Epoch 82000/100000\n",
      "The training loss is 10.39948142899057. The test loss is 11.792477627288319.\n",
      "\n",
      "Epoch 82500/100000\n",
      "The training loss is 10.39948142894137. The test loss is 11.792477170716117.\n",
      "\n",
      "Epoch 83000/100000\n",
      "The training loss is 10.399481428898145. The test loss is 11.792476742779138.\n",
      "\n",
      "Epoch 83500/100000\n",
      "The training loss is 10.399481428860172. The test loss is 11.792476341681274.\n",
      "\n",
      "Epoch 84000/100000\n",
      "The training loss is 10.399481428826814. The test loss is 11.792475965739074.\n",
      "\n",
      "Epoch 84500/100000\n",
      "The training loss is 10.399481428797511. The test loss is 11.792475613374709.\n",
      "\n",
      "Epoch 85000/100000\n",
      "The training loss is 10.399481428771761. The test loss is 11.792475283109335.\n",
      "\n",
      "Epoch 85500/100000\n",
      "The training loss is 10.399481428749146. The test loss is 11.792474973556878.\n",
      "\n",
      "Epoch 86000/100000\n",
      "The training loss is 10.399481428729269. The test loss is 11.7924746834182.\n",
      "\n",
      "Epoch 86500/100000\n",
      "The training loss is 10.399481428711816. The test loss is 11.79247441147569.\n",
      "\n",
      "Epoch 87000/100000\n",
      "The training loss is 10.399481428696479. The test loss is 11.792474156588074.\n",
      "\n",
      "Epoch 87500/100000\n",
      "The training loss is 10.399481428683007. The test loss is 11.792473917685696.\n",
      "\n",
      "Epoch 88000/100000\n",
      "The training loss is 10.399481428671171. The test loss is 11.792473693765997.\n",
      "\n",
      "Epoch 88500/100000\n",
      "The training loss is 10.399481428660774. The test loss is 11.792473483889278.\n",
      "\n",
      "Epoch 89000/100000\n",
      "The training loss is 10.399481428651638. The test loss is 11.792473287174792.\n",
      "\n",
      "Epoch 89500/100000\n",
      "The training loss is 10.399481428643611. The test loss is 11.792473102797073.\n",
      "\n",
      "Epoch 90000/100000\n",
      "The training loss is 10.399481428636566. The test loss is 11.79247292998238.\n",
      "\n",
      "Epoch 90500/100000\n",
      "The training loss is 10.399481428630372. The test loss is 11.792472768005515.\n",
      "\n",
      "Epoch 91000/100000\n",
      "The training loss is 10.39948142862493. The test loss is 11.792472616186775.\n",
      "\n",
      "Epoch 91500/100000\n",
      "The training loss is 10.399481428620149. The test loss is 11.792472473889099.\n",
      "\n",
      "Epoch 92000/100000\n",
      "The training loss is 10.39948142861595. The test loss is 11.792472340515355.\n",
      "\n",
      "Epoch 92500/100000\n",
      "The training loss is 10.399481428612264. The test loss is 11.792472215505887.\n",
      "\n",
      "Epoch 93000/100000\n",
      "The training loss is 10.39948142860902. The test loss is 11.792472098336123.\n",
      "\n",
      "Epoch 93500/100000\n",
      "The training loss is 10.399481428606176. The test loss is 11.792471988514407.\n",
      "\n",
      "Epoch 94000/100000\n",
      "The training loss is 10.39948142860367. The test loss is 11.792471885579905.\n",
      "\n",
      "Epoch 94500/100000\n",
      "The training loss is 10.399481428601474. The test loss is 11.792471789100691.\n",
      "\n",
      "Epoch 95000/100000\n",
      "The training loss is 10.399481428599543. The test loss is 11.792471698671926.\n",
      "\n",
      "Epoch 95500/100000\n",
      "The training loss is 10.399481428597849. The test loss is 11.792471613914161.\n",
      "\n",
      "Epoch 96000/100000\n",
      "The training loss is 10.399481428596358. The test loss is 11.792471534471765.\n",
      "\n",
      "Epoch 96500/100000\n",
      "The training loss is 10.399481428595049. The test loss is 11.792471460011367.\n",
      "\n",
      "Epoch 97000/100000\n",
      "The training loss is 10.399481428593901. The test loss is 11.79247139022055.\n",
      "\n",
      "Epoch 97500/100000\n",
      "The training loss is 10.399481428592892. The test loss is 11.792471324806455.\n",
      "\n",
      "Epoch 98000/100000\n",
      "The training loss is 10.399481428592004. The test loss is 11.792471263494615.\n",
      "\n",
      "Epoch 98500/100000\n",
      "The training loss is 10.399481428591223. The test loss is 11.792471206027756.\n",
      "\n",
      "Epoch 99000/100000\n",
      "The training loss is 10.399481428590539. The test loss is 11.792471152164751.\n",
      "\n",
      "Epoch 99500/100000\n",
      "The training loss is 10.399481428589937. The test loss is 11.792471101679604.\n",
      "\n",
      "Epoch 100000/100000\n",
      "The training loss is 10.399481428589409. The test loss is 11.792471054360465.\n",
      "\n",
      "Epoch 100500/100000\n",
      "The training loss is 10.399481428588942. The test loss is 11.792471010008788.\n"
     ]
    }
   ],
   "source": [
    "# compute w and calculate its performance\n",
    "w_num = get_w_numerical(X_train_bias,y_train,X_test_bias,y_test,100000,1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, then your loss should be going down with each epoch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q: How do these results compare to those of the analytical solution?**\n",
    "\n",
    "**Q: In which cases may it be preferable to use the numerical approach over the analytical solution?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Using sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the sklearn implementation of the linear regression model. sklearn is a library that contains implementations of many popular machine learning models, including the linear regression model!\n",
    "\n",
    "Please look up the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to \n",
    "\n",
    "1. instantiate the LinearRegression model\n",
    "2. fit the model to our training data\n",
    "3. evaluate the model on the test data\n",
    "4. and compare the results with our previous outcomes\n",
    "\n",
    "Especially check out the example code they provide!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of sklearn linear regression model on test data:  11.792470347125798\n",
      "CPU times: user 3.82 ms, sys: 943 µs, total: 4.76 ms\n",
      "Wall time: 4.01 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "print('MSE of sklearn linear regression model on test data: ' , metrics.mean_squared_error(y_test,y_hat))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
