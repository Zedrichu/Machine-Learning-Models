{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "752c8f3f",
   "metadata": {},
   "source": [
    "# Exercise Session 7 - Cross-validation\n",
    "\n",
    "How should we be choosing the value of $k$ for kNN? If we choose the $k$ that gives us the highest **test** accuracy, we would be **cheating** because we would be tuning our model to its test data. \n",
    "\n",
    "In practice, we choose the $k$ that gives us the highest **validation** accuracy, via the cross-validation method. By doing so, we ensure that we select a method which generalizes well to unseen test data.\n",
    "\n",
    "In this little helper notebook, you will see some pointers to help you implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2290eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from helpers import KNN, mse_fn, macrof1_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969a1f6",
   "metadata": {},
   "source": [
    "## 1.  K-Fold Cross Validation \n",
    "\n",
    "K-fold is a type of cross validation technique that consists into dividing the dataset into K parts (:= folds) of equal size. Each fold is considered the validation set of the rest of the folds, which are used for training.\n",
    "It works in the following way:\n",
    "\n",
    "1 - Split the training data in K folds. Select 1 fold as our validation set and the rest as our training set.\n",
    "\n",
    "2 - Train our model on the training set and find the accuracy (or other metric value) of the validation set. \n",
    "\n",
    "3 - Repeat steps 1&2 K times, each time selecting a different fold of the data for the validation set. \n",
    "\n",
    "4 - In the end we will find K different validation accuracies that we average. This will represent the accuracy/performance of our model. (See the image below).\n",
    "\n",
    "![](cross_validation.png)\n",
    "\n",
    "This process can be used to choose the best hyper-parameter value for a given model. Indeed, we can repeat it for different values, tracking which one gives the best accuracy/performance. For a kNN example:\n",
    "\n",
    "a - Repeat steps 1-4 (the whole process) for different $k$ values (hyperparameter for kNN). \n",
    "\n",
    "b - Find the $k$ value that gave the highest validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521e2bbb",
   "metadata": {},
   "source": [
    "### 1.1 Splitting the data\n",
    "First, you need to implement `splitting_fn()`, a function that split the data into the specific training and validation folds, e.g. the 4th one on the graph above.\n",
    "\n",
    "To do the splitting, we first do the following for you:\n",
    "\n",
    "a - Create an array of indices from 0 to N-1, where N is the number of data points.\n",
    "\n",
    "b - Shuffle these indices. This is useful to effectively shuffles the data beforehand.\n",
    "\n",
    "Then, you will have to implement the following steps in your function:\n",
    "\n",
    "1 - Extract the indices of the *validation* data for the given `fold`. (Corresponding to the blue square in the image above, which should have a size of `fold_size`.)\n",
    "\n",
    "2 - Extract the indices of the *training* data for the given `fold`. (Corresponding to the gray square in the image above.) **Helper:** you can take a look at `np.setdiff1d`.\n",
    "\n",
    "3 - Select all the data points corresponding to the training indices, storing them into `train_data`. Do the same for their labels in `train_label`.\n",
    "\n",
    "4 - Select all the data points corresponding to the validation indices, storing them into `val_data`. Do the same for their labels in `val_label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5b3a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_fn(data, labels, indices, fold_size, fold):\n",
    "    \"\"\"\n",
    "        Function to split the data into training and validation folds.\n",
    "        Arguments:\n",
    "            data (np.array, of shape (N, D)): data (which will be split to training \n",
    "                and validation data during cross validation),\n",
    "            labels  (np.array, of shape (N,)): the labels of the data\n",
    "            indices: (np.array, of shape (N,)): array of pre shuffled indices (integers ranging from 0 to N)\n",
    "            fold_size (int): the size of each fold\n",
    "            fold (int): the index of the current fold.\n",
    "        Returns:\n",
    "            train_data, train_label, val_data, val_label (np. arrays): split training and validation sets\n",
    "    \"\"\"\n",
    "    trainslice = np.setdiff1d(np.arange(data.shape[0]), np.arange(fold_size*fold,fold_size*(fold+1)))\n",
    "    \n",
    "    train_data = data[indices[trainslice], :]\n",
    "    train_label = labels[indices[trainslice]]\n",
    "    val_data = data[indices[fold * fold_size : (fold+1) * fold_size], :]\n",
    "    val_label = labels[indices[fold * fold_size : (fold+1) * fold_size]]\n",
    "\n",
    "    return train_data, train_label, val_data, val_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8ead5",
   "metadata": {},
   "source": [
    "The following cell is a small test to partially verify the function.\n",
    "\n",
    "Note: you are encouraged to test it further yourself. What do you expect the outputs to be? Does what you obtain make sense to you? Don't hesitate to print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0dfa2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "k_fold = 5  # the number of folds\n",
    "data = np.arange(N).reshape(N,1)\n",
    "labels = (data >= 4).astype(int).reshape(N)\n",
    "\n",
    "indices = np.arange(N)  # array on indices from 0 to N-1\n",
    "np.random.shuffle(indices)  # we shuffle that array\n",
    "fold_size = N//k_fold\n",
    "\n",
    "for fold in range(k_fold):\n",
    "    train_data, train_label, val_data, val_label = splitting_fn(data, labels, indices, fold_size, fold)\n",
    "\n",
    "    if not all([x not in train_data for x in val_data]):\n",
    "        print(\"There seems to be an error in your splitting_fn: train_data and val_data share some data points!\")\n",
    "        break\n",
    "    \n",
    "\n",
    "    cat_data = np.concatenate([train_data, val_data], axis=0)\n",
    "    if not all([x in cat_data for x in data]):\n",
    "        print(\"There seems to be an error in your splitting_fn: not all data points are used!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9fc0f",
   "metadata": {},
   "source": [
    "### 1.2 Performing K-Fold Cross Validation\n",
    "\n",
    "You need to implement the main function  `cross_validation`.\n",
    "The aim is to perform such cross-validation over a specific hyper-parameter of the method, given some data and labels.\n",
    "\n",
    "Again, here are the steps:\n",
    "\n",
    "1. Repeat for the different parameter values:\n",
    "    1. Repeat K times, each time selecting a different fold of the data for the validation set:\n",
    "        1. Split the training data in K folds. Select 1 fold as our validation set and the rest as our training set.\n",
    "        2. Train our model on the training set and find the accuracy (or other metric value) of the validation set. \n",
    "    2. In the end we will find K different validation accuracies that we average. This will represent the accuracy/performance of our model with that parameter.\n",
    "3. Select the value that gave the highest validation accuracy.\n",
    "\n",
    "\n",
    "**Note:**\n",
    "* We talked about accuracy, however it can be generalized to any metric! For example, in the case of regression we are looking at MSE, and we want to minimize it.\n",
    "    * You can use `metric` and `find_param_ops` below for this.\n",
    "* We create and shuffle the indices for you, in `indices`.\n",
    "* To track the metric for each parameter value, you can use `acc_list1`.\n",
    "* To track the metric for each fold, you can use `acc_list2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "449f92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(method_obj=None, search_arg_name=None, search_arg_vals=[], data=None, labels=None, k_fold=4):\n",
    "    \"\"\"\n",
    "        Function to run cross validation on a specified method, across specified arguments.\n",
    "        Arguments:\n",
    "            method_obj (object): A classifier or regressor object, such as KNN. Needs to have\n",
    "                the functions: set_arguments, fit, predict.\n",
    "            search_arg_name (str): the argument we are trying to find the optimal value for\n",
    "                for example, for DummyClassifier, this is \"dummy_arg\".\n",
    "            search_arg_vals (list): the different argument values to try, in a list.\n",
    "                example: for the \"DummyClassifier\", the search_arg_name is \"dummy_arg\"\n",
    "                and the values we try could be [1,2,3]\n",
    "            data (np.array, of shape (N, D)): data (which will be split to training \n",
    "                and validation data during cross validation),\n",
    "            labels  (np.array, of shape (N,)): the labels of the data\n",
    "            k_fold (int): number of folds\n",
    "        Returns:\n",
    "            best_hyperparam (float): best hyper-parameter value, as found by cross-validation\n",
    "            best_acc (float): best metric, reached using best_hyperparam\n",
    "    \"\"\"\n",
    "    ## choose the metric and operation to find best params based on the metric depending upon the\n",
    "    ## kind of task.\n",
    "    metric = mse_fn if method_obj.task_kind == 'regression' else macrof1_fn\n",
    "    find_param_ops = np.argmin if method_obj.task_kind == 'regression' else np.argmax\n",
    "\n",
    "    N = data.shape[0]\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    fold_size = N//k_fold\n",
    "\n",
    "    acc_list1 = []\n",
    "    for arg in search_arg_vals:\n",
    "        arg_dict = {search_arg_name: arg}\n",
    "        # this is just a way of giving an argument \n",
    "        # (example: for DummyClassifier, this is \"dummy_arg\":1)\n",
    "        method_obj.set_arguments(**arg_dict)\n",
    "\n",
    "        acc_list2 = []\n",
    "        for fold in range(k_fold):\n",
    "            train_data, train_label, val_data, val_label = splitting_fn(data, labels, indices, fold_size, fold) \n",
    "            method_obj.fit(train_data, train_label)\n",
    "            predictions = method_obj.predict(val_data)\n",
    "            acc_list2.append(metric(predictions, val_label))\n",
    "        \n",
    "        acc_list1.append(np.mean(acc_list2))\n",
    "     \n",
    "    best_index = np.argmax(acc_list1)\n",
    "    best_hyperparam = search_arg_vals[best_index]\n",
    "    best_acc = acc_list1[best_index]\n",
    "\n",
    "    return best_hyperparam, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f612c05c",
   "metadata": {},
   "source": [
    "We test the function in the following cell.\n",
    "\n",
    "Note: as a bit of randomness is involved (the shuffling of the indices), you might want to run it a few times to verify if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "27d03439",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_obj = KNN()\n",
    "search_arg_name = \"k\"\n",
    "search_arg_vals = [11, 101, 501]\n",
    "\n",
    "data = np.load(\"./hbody_feats_annotated.npy\")\n",
    "labels = np.load(\"./hbody_labels_annotated.npy\")\n",
    "k_fold = 4\n",
    "\n",
    "best_hyperparam, best_acc = cross_validation(method_obj, search_arg_name, search_arg_vals, data, labels, k_fold)\n",
    "\n",
    "if best_hyperparam != 11:\n",
    "    print(\"There seems to be an error in your cross_validation: \"\n",
    "          \"given this setup, we expect k=11 to be better than 101 and 501!\")\n",
    "    \n",
    "if best_acc < 0.79:\n",
    "    print(\"There seems to be an error in your cross_validation: \"\n",
    "          \"given this setup, we expect the macrof1 score to be above 0.79!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
