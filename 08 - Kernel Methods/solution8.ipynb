{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Session 7 - Kernel SVM\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "Welcome to the 7th exercise session of CS233 - Introduction to Machine Learning.  \n",
    "\n",
    "We will continue using scikit-learn, which can be reviewed from the SVM exercise (week 5).\n",
    "\n",
    "Let us use it to train SVMs with feature expansion and different kernel functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from plots import plot, plot_expand, plot_expand_poly, plot_mykernel\n",
    "from sklearn import svm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the linear problem we discussed before, SVM can also solve non-linear classification problems by using kernel functions. We replace $\\mathbf{x}_i$ with $\\phi(\\mathbf{x}_i)$, and then $\\mathbf{x}_i^T\\mathbf{x}_j$ with $k(\\mathbf{x}_i,\\mathbf{x}_j)$. The **dual form** of the SVM training problem is then given by:  \n",
    "\\begin{align}\n",
    "    \\underset{\\{\\alpha_i\\}}{\\operatorname{max}} \\ \\ \n",
    "    & \\sum_{i=1}^N \\alpha_i - \\frac 1 2 \\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i\\alpha_jy_iy_jk(\\mathbf{x}_i,\\mathbf{x}_j)  \\\\   \n",
    "    \\operatorname{subject \\ to} & \\ \\ \\sum_{i=1}^N \\alpha_iy_i = 0 \\\\\n",
    "                 & \\ \\ 0 \\leq \\alpha_i \\leq C, \\forall i \\ \\ \n",
    "\\end{align}\n",
    "**Questions**\n",
    "   * How can you write $\\tilde{\\mathbf{w}}$ using the $\\alpha_i$s and the function $\\phi$?\n",
    "   * How is $y(\\mathbf{x})$ represented using the $\\alpha_i$s?\n",
    "\n",
    "\n",
    "**Answers**\n",
    "   * $\\tilde{\\mathbf{w}} = \\sum_{i=1}^N \\alpha_iy_i\\phi(x_i) $\n",
    "   * Replacing the definition of $\\tilde{\\mathbf{w}}$ in the prediction equation yields \n",
    "     \\begin{align}\n",
    "       \\hat{y}(\\mathbf{x}) &= \\tilde{\\mathbf{w}}^T\\phi(\\mathbf{x}) + w_0 \\\\\n",
    "                           &= \\sum_{i=1}^N \\alpha_iy_ik(\\mathbf{x}_i,\\mathbf{x}) + w_0\n",
    "     \\end{align}\n",
    "   * The sum can be computed on the support vectors (${\\cal S}$) only, i.e., \n",
    "       $\\begin{align}\n",
    "         \\hat{y}(\\mathbf{x}) & = \\sum_{i \\in {\\cal S}} \\alpha_iy_ik(\\mathbf{x}_i,\\mathbf{x}) + w_0\n",
    "     \\end{align}$\n",
    "   \n",
    "Have a look at the SVM function [here.](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) The main parameters you should look for are:\n",
    "- Kernel Function: Linear, Polynomial and RBF \n",
    "    - Linear: `linear` $\\langle {\\bf x}_i, {\\bf x}_j \\rangle $.\n",
    "    - Polynomial: `poly` $( \\gamma \\langle {\\bf x}, {\\bf x}' \\rangle + c)^d $. $d$ is specified by the keyword `degree`, $c$ by `coef0`.\n",
    "    - RBF: `rbf` $\\exp(- \\gamma \\|{\\bf x} - {\\bf x}'\\|^2)$. $\\gamma$ is specified by the keyword `gamma` and must be greater than 0.\n",
    "    \n",
    "    Above, ${\\bf x}$ and ${\\bf x}'$ are data samples, and $\\langle \\cdot, \\cdot \\rangle$ indicates a dot product.\n",
    "- Penalty term: C \n",
    "- Gamma: for the RBF and polynomial kernel\n",
    "- Degree: for the polynomial kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Linearly-separable data\n",
    "\n",
    "Let us look back at the linearly-separable data from exercise 5. This time, let's use an SVM again but with different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_simple_dataset\n",
    "\n",
    "# Get the simple dataset\n",
    "X_linear, Y_linear = get_simple_dataset()\n",
    "plot(X_linear, Y_linear, None, dataOnly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Linear SVM\n",
    "Firstly, we can use a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an SVM with a linear kernel and set C=0.1\n",
    "# You can also try some other C values and see what happens\n",
    "clf = svm.SVC(kernel='linear', C=0.1)\n",
    "\n",
    "# Call the fit method\n",
    "clf.fit(X_linear, Y_linear)\n",
    "\n",
    "# Plot the decision boundary and the support vectors\n",
    "plot(X_linear, Y_linear, clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the decision boundary and margins of the learnt model. The circled points are the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Polynomial and RBF kernels \n",
    "We can then try to use Polynomial and RBF kernels. Have a look at the shape of the resulting decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an SVM with a polynomial kernel and set C=0.1, degree=2\n",
    "# You can also try some other C and degree values and see what happens\n",
    "\n",
    "### CODE HERE ###\n",
    "clf = svm.SVC(kernel='poly', C=0.1, degree=2, gamma=1.0, coef0=1.0)\n",
    "\n",
    "clf.fit(X_linear, Y_linear)\n",
    "plot(X_linear, Y_linear, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an SVM with an RBF kernel and set C=0.1, gamma=0.1 , \n",
    "# You can also try some other C and gamma values and see what happens\n",
    "\n",
    "### CODE HERE ###\n",
    "clf = svm.SVC(kernel='rbf', C=0.1, gamma=0.1)\n",
    "\n",
    "clf.fit(X_linear, Y_linear)\n",
    "plot(X_linear, Y_linear, clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is linearly separable, kernel methods are unnecessary in this case. How about for non-linearly separable data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Non-Linearly separable data\n",
    "Until now we have worked with linearly-seperable data in the input feature space. Most of the time, this won't be the case. Let us now look at non-linear data and use non-linear kernels classify it. These kernels implicitly project the data to a higher dimensional space where the classes can be linearly separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_circle_dataset\n",
    "\n",
    "X,Y = get_circle_dataset()\n",
    "plot(X,Y,None,dataOnly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Linear SVM\n",
    "Is a linear SVM able to classify the data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a linear SVM\n",
    "### CODE HERE ###\n",
    "clf_linear = svm.SVC(kernel='linear', C=0.1)\n",
    "    \n",
    "clf_linear.fit(X, Y)\n",
    "plot(X, Y, clf_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVM fails on this non-linear dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Polynomial feature expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to perform an explicit polynomial feature expension to project the data to higher dimension so that it can be classified linearly. \n",
    "\n",
    "Fill in the function `expand_X()`. You should add a bias term, but **omit the interaction terms**. For example:\n",
    "\n",
    "For $D=2$, $\\text{degree_of_expansion}=2$ you have:\n",
    "$$\n",
    "\\mathbf{x_i} = \\begin{bmatrix}\\mathbf{x}_i^{(1)}& \\mathbf{x}_i^{(2)}\\end{bmatrix}\n",
    "$$\n",
    "After the polynomial feature expansion, you would like to have:\n",
    "$$ \n",
    "\\mathbf{\\phi}(\\mathbf{x}_i) = \\begin{bmatrix}1 & \\mathbf{x}_i^{(1)} & \\mathbf{x}_i^{(2)} & (\\mathbf{x}_i^{(1)})^2 & (\\mathbf{x}_i^{(2)})^2 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_X(X, degree_of_expansion):\n",
    "    \"\"\"  Perform degree-d polynomial feature expansion of X, \n",
    "         with bias but omitting the interaction terms\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): data, shape (N, D).\n",
    "        degree_of_expansion (int): The degree of the polynomial feature expansion.\n",
    "    \n",
    "    Returns:\n",
    "        (np.array): Expanded data of shape (N, new_D), \n",
    "                    where new_D is D*degree_of_expansion+1\n",
    "    \n",
    "    \"\"\"\n",
    "    expanded_X = np.ones((X.shape[0],1))\n",
    "    ### CODE HERE ###\n",
    "    for idx in range(1,degree_of_expansion+1): \n",
    "        expanded_X = np.hstack((expanded_X, X**idx))  \n",
    "    return expanded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial feature expansion without the interaction terms\n",
    "degree_of_expansion = 2\n",
    "expanded_X = expand_X(X, degree_of_expansion)\n",
    "\n",
    "print(\"The original data has {} features.\".format(X.shape[1]))\n",
    "print(\"After degree-{} polynomial feature expansion (with bias, without interaction terms) the data has {} features.\".format(degree_of_expansion,expanded_X.shape[1]))\n",
    "\n",
    "# Use a linear SVM on the expanded features with C=10.0\n",
    "### CODE HERE ###\n",
    "expanded_clf = svm.SVC(kernel='linear', C=10.0)\n",
    "expanded_clf.fit(expanded_X, Y)\n",
    "\n",
    "plot_expand(X, Y, expanded_clf, degree_of_expansion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the non-linear data can be classified by a linear SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Polynomial and RBF Kernels \n",
    "\n",
    "Now, we will use polynomial and RBF kernels. First, let's implement the polynomial kernel, which is defined as 'poly' above.\n",
    "\n",
    "- poly: $( \\gamma \\langle X, X' \\rangle + r)^d $. $d$ is specified by keyword `degree`, $r$ by `coef0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute your own polynomial kernel\n",
    "def my_poly_kernel(X, Xprime, degree=3, gamma=1.0, coef0=1.0):\n",
    "    \"\"\"  Compute a degree-d polynomial kernel matrix between two sets of data X and Xprime \n",
    "    Args:\n",
    "        X (np.array): data, shape (N1, D).\n",
    "        Xprime (np.array): data, shape (N2, D).\n",
    "        degree (int): The degree of the polynomial kernel method.\n",
    "    Returns:\n",
    "        K (np.array): the kernel matrix from data matrices; that matrix should be an array of shape (N1, N2).    \n",
    "    \"\"\"\n",
    "    ### CODE HERE ###\n",
    "    K = (gamma * np.dot(X, Xprime.T) + coef0) ** degree   \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the polynomial feature expansion with **the interaction terms** by using [sklearn.preprocessing.PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question a**  What are the differences between polynomial feature expansion and a polynomial kernel? \n",
    "\n",
    "**Question b**  Is the SVM trained with a linear kernel on the polynomial-expanded data the same as the SVM trained with a polynomial kernel on the original data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check our implementation of polynomial kernel and find out the answer to Question b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These three SVMs should be same.\n",
    "\n",
    "degree_of_expansion = 3\n",
    "\n",
    "# Baseline: \n",
    "# Use an SVM with a polynomial kernel on the original features with C=10.0, gamma=1.0, coef0=1.0\n",
    "### CODE HERE ###\n",
    "clf = svm.SVC(kernel='poly', C=10., degree=degree_of_expansion, gamma=1.0, coef0=1.0)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "plot(X, Y, clf)\n",
    "\n",
    "# Use an SVM with you own kernel function (my_poly_kernel), which should give the same results as the previous method when using same parameters\n",
    "### CODE HERE ###\n",
    "    # tip: kernel=my_poly_kernel\n",
    "kernel_clf = svm.SVC(kernel=my_poly_kernel, C=10.)\n",
    "kernel_clf.fit(X, Y)\n",
    "\n",
    "plot_mykernel(X, Y, kernel_clf)\n",
    "\n",
    "# Use an SVM with a linear kernel on the expanded features with C=10.0\n",
    "## Use PolynomialFeatures to generate the expanded features with the interaction terms\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "### CODE HERE ###\n",
    "poly = PolynomialFeatures(degree_of_expansion)\n",
    "poly_expanded_X = poly.fit_transform(X)\n",
    "\n",
    "print(\"The original data has {} features.\".format(X.shape[1]))\n",
    "print(\"After degree-{} polynomial feature expansion (with bias, with interaction terms) the data has {} features.\".format(degree_of_expansion,poly_expanded_X.shape[1]))\n",
    "\n",
    "## Use an SVM with a linear kernel on the expanded features with C=10.0\n",
    "### CODE HERE ###\n",
    "poly_expanded_clf = svm.SVC(kernel='linear', C=10.0)\n",
    "poly_expanded_clf.fit(poly_expanded_X, Y)\n",
    "\n",
    "plot_expand_poly(X, Y, poly_expanded_clf, degree_of_expansion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer a** Polynomial feature expansion is applied on each sample while the polynomial kernel function directly computes the dot products between the polynomial-expanded features of each pair of samples in the training set. \n",
    "\n",
    "**Answer b** Yes, the 2 SVMs are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with different settings for different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an SVM with a polynomial kernel of different degrees\n",
    "D = 2 ** np.linspace(0, 6, num=7)\n",
    "for d in D:\n",
    "    ## use poly kernel with C=10., gamma=1.0, coef0=1.0 and different degrees\n",
    "    ### CODE HERE ###\n",
    "    clf = svm.SVC(kernel='poly', C=10., degree=d, gamma=1.0, coef0=1.0)\n",
    "    \n",
    "    clf.fit(X, Y)\n",
    "    plot(X, Y, clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an SVM with an RBF Kernel with differen gammas\n",
    "G = np.logspace(-2,2,num=5)\n",
    "for g in G:\n",
    "    ## use rbf kernel with C=0.1 and different gammas\n",
    "    ### CODE HERE ###\n",
    "    clf = svm.SVC(kernel='rbf', C=0.1, gamma=g)\n",
    "    \n",
    "    clf.fit(X, Y)\n",
    "    plot(X, Y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!!** It is important to choose appropriate parameters for the kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** For the given dataset, which kernel would you consider to be the best one? If you choose the polynomial kernel, which degree would be the best? If you choose the RBF kernel, which gamma would be the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Let's use Grid Search and Cross Validation techniques to find out the answer. We can apply K-Fold cross validation to different sets of parameters. We then use the mean accuracy to select the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at K-Fold cross validation for the RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold cross validation for searching for the parameters gamma and C of an SVM based on an RBF kernel.\n",
    "from helpers import do_cross_validation, fold_indices\n",
    "\n",
    "# seach in log space\n",
    "grid_search_c = np.logspace(-4, 10, num=15)\n",
    "grid_search_gamma = np.logspace(-9, 5, num=15)\n",
    "\n",
    "#save the accuracies for all combinations of hyperparameters\n",
    "grid_val = np.zeros((len(grid_search_c), len(grid_search_gamma)))\n",
    "\n",
    "# Perform 4 fold cross validation\n",
    "k_fold = 4\n",
    "k_fold_ind = fold_indices(X.shape[0], k_fold)\n",
    "\n",
    "for i, c in enumerate(grid_search_c):\n",
    "    for j, g in enumerate(grid_search_gamma):\n",
    "        print('Evaluating for C:{} gamma:{} ...'.format(c, g))\n",
    "        \n",
    "        ## call SVM with c,g as params.\n",
    "        ### CODE HERE ####\n",
    "        clf = svm.SVC(kernel='rbf', C=c, gamma=g)\n",
    "        \n",
    "        acc = np.zeros(k_fold)\n",
    "        ## do cross validation\n",
    "        for k in range(k_fold):\n",
    "            acc[k] = do_cross_validation(clf, k, k_fold_ind, X, Y)\n",
    "            \n",
    "        ## fill out the grid_val by computing the mean accuracy from k_fold runs.\n",
    "        ### CODE HERE ####\n",
    "        grid_val[i,j] = np.mean(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import plot_cv_result_rbf\n",
    "## show all results and the best one\n",
    "plot_cv_result_rbf(grid_val,grid_search_c,grid_search_gamma)\n",
    "print('Best acc:{}'.format(np.max(grid_val)))\n",
    "## best params\n",
    "cin,gin = np.unravel_index(np.argmax(grid_val),grid_val.shape)\n",
    "print('Best Params- C:{}, Gamma:{}'.format(grid_search_c[cin],grid_search_gamma[gin]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above heatmap shows the accuracies for different gamma and C values. The best parameter values are used on the test set.   \n",
    "**Question** Is there a relation between C and gamma?   \n",
    "**Hint**: Look at how increasing one value changes the other in the heatmap. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: A high gamma will lead to overfitting, hence a smaller C would allow for more misclassification to counteract this phenomenon. A lower gamma leads to underfitting, and one can thus use a high C  to prevent misclassifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now perform K-Fold cross validation for the polynominal kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold cross validation for searching for the degree and parameter C of an SVM based on an RBF kernel.\n",
    "from helpers import do_cross_validation, fold_indices\n",
    "\n",
    "grid_search_c = np.logspace(-5,5,num=11)\n",
    "grid_search_degree = 2 ** np.linspace(0, 8, num=9)\n",
    "\n",
    "#save the accuracies for all combinations of hyperparameters\n",
    "grid_val = np.zeros((len(grid_search_c),len(grid_search_degree)))\n",
    "\n",
    "# Perform 4 fold cross validation\n",
    "k_fold = 4\n",
    "k_fold_ind = fold_indices(X.shape[0],k_fold)\n",
    "\n",
    "for i, c in enumerate(grid_search_c):\n",
    "    for j, d in enumerate(grid_search_degree):\n",
    "        print('Evaluating for C:{} degree:{} ...'.format(c, d))\n",
    "        \n",
    "        ## call SVM with c,d as params.\n",
    "        ### CODE HERE ####\n",
    "        clf = svm.SVC(kernel='poly', C=c, degree=d, gamma=1.0, coef0=1.0)\n",
    "        \n",
    "        acc = np.zeros(k_fold)\n",
    "        # do cross validation\n",
    "        for k in range(k_fold):\n",
    "            acc[k] = do_cross_validation(clf,k,k_fold_ind,X,Y)\n",
    "            \n",
    "        ## fill out the grid_val by computing the mean accuracy from k_fold runs. \n",
    "        ### CODE HERE ####\n",
    "        grid_val[i,j] = np.mean(acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import plot_cv_result_poly\n",
    "## show all results and the best one\n",
    "plot_cv_result_poly(grid_val, grid_search_c, grid_search_degree)\n",
    "print('Best acc:{}'.format(np.max(grid_val)))\n",
    "## best params\n",
    "cin,gin = np.unravel_index(np.argmax(grid_val),grid_val.shape)\n",
    "print('Best Params- C:{}, Degree:{}'.format(grid_search_c[cin],grid_search_degree[gin]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
